Greek word of "Helmsman"
Inspired by google's Borg project
Written in Golang
Alternate tools for Kubernetes:
  Msos
  Docker Swarm

Master components
  apiserver
  etcd  - metadata key-value pair data base, which has the information of every container in the nodes
  scheduler
  controllers

In kubernetes we call containers as pods.
Pod is a wrapper on top of container

Prerequisites:
  1. nodes should have a hostname nothing but DNS (#cat /etc/hosts) if not setup use following command to setup #hostnamectl
  2. ip should be static  ()
  3. ip to name and name to ip should be resolve
  4. swap should be off (check it by #free -m)
  5. docker should be up and running  (systemctl status docker)
  5. kube repo should be there  (cat /etc/apt/sources.list)
      deb http://apt.kubernetes.io/ kubernetes-xenial main
  6. kubelet service must be enabled

* Kublet is agent in every node
* The master server has kub-api server and that is what it makes as master
* And worker nodes has kublet agent that is responsible to interact with a master to give healthier info of worker node and carry out actions requested by the master on the worker nodes
* Kubectl also called as kube-control
* Kubectl tool is used to deploy or manage applications on the cluster
* Different setups:
    1. Mini kube is a tool used to setup a single instance of kubernates in an all in one setup
    2. Kubeadmin is a tool to configure kubernates in multi node setup on our local machines
* label in pod addresses the behaviour of the pod
* NameSpace : Isolate resources but level can same. we have two pods with same name but in two diff namespaces. so namespaces are just to differentiate the diff projects
* Pod cannot be accessible outside the cluster at all
* Service: is a kubernetes object which does load-balancing. Before load balancing it creates a group of pods that work together
  Types: LoadBalancer, NodePort, ClusterIP




Make a node as Kubeadm Master
  Follow the commands in the Day1 document



Commands
#name: cleanup kubernetes data
#shell: |
  kubeadm reset -f
  systemctl stop kubelet
  systemctl stop docker
  systemctl stop etcd
  systemctl stop keepalived
  yum remove -y docker-ce*
  rm -rf /var/lib/cni/
  rm -rf /var/lib/kubelet/*
  rm -rf /etc/keepalived/*
  rm -rf /etc/cni/
  rm -r /var/lib/etcd/member
  ip link delete cni0
  ip link delete flannel.1
  rm -rf /var/lib/docker
  rm /usr/local/bin/etcd
  rm /usr/local/bin/etcdctl
  rm /usr/local/bin/cfssljson
  rm /usr/local/bin/cfssl
  rm -rf /etc/kubernetes

kubeadm version
kubeadm init --apiserver-advertise-address $(hostname -i)
kubeadm reset
kubectl get nodes
kubectl get pods --all-namespaces -w
  kubectl apply -n kube-system -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"    (for applying network plugin)
  kubectl get pods -n kube-system
  kubectl get pods --all-namespaces -o wide
  can we reset and init with pod-network-cidr option
kubeadm token create --print-join-command
  kubeadm join 172.31.25.15:6443 --token pa5vzp.thfqxclp95cbmnhx     --discovery-token-ca-cert-hash sha256:e59dd880df0130909a6482506ea980a7be72b80e7f1055efe7ca78a69cf33ff0   (this is join command looks)
systemctl status kubelet
kubectl get pods
kubectl explain pod (to check requirements to prepare yaml file for pod setup)
kubectl api-versions | less  (to check api versions each resource)
vi pod.yml
------------------------------------------
apiVersion: v1
kind: Pod
metadata:
 name: mypod1
 labels:
  name: myapp1
spec:
 containers:
 - name: mycontainer
   image: httpd
   ports:
   - containerPort: 80
-------------------------------------------------
  kubectl create -f pod.yml
  kubectl get pods -w
  kubectl get pods --show-labels
  kubectl get pods -o wide    (to check in which slave does it running)
  curl <IP of pod running node>   (to check whether deployed application is running or not, also we check the running state of the application in any node)
  kubectl describe pod <pod_name>   (give detailed description of running pod) (here take the container id and check the status in the slave node)
  docker ps | grep <first_five_digits_container>
kubectl exec -it <pod_name> bash    (to enter into the pod)
  cat htdocs/index.html
  echo "hello all" > htdocs/index.html
  cat htdocs/index.html
  exit
curl <IP of pod running node>
we can create another pod following the same procedure but pod name should modified in the yml file and label can be remain same as per the task - follow line 80. Each pod has its own IP address

Namespces: till now we have worked on default namespace and now lets create our own namespace
  kubectl get namespaces
  kubectl create namespace <namespace_name>
  kubectl get pods -n <namespace_name>    (it says no resources as no pod has been created yet in this namespace)
  kubectl create -f pod.yml -n <namespace_name or project_name>
  kubectl get pods -n <namespace_name>    (now it will show the resources)
  kubectl get pods --all-namespaces

Class#2
for labeling the pod there is no thumb rule to follow only this
  labels:
    name: myapp1
We can also give like below
  labels:
    color: red

Services:
  kubectl get svc
  kubectl explain service
  vi svc.yml
  ----------------------------------
  apiVersion: v1
  kind: Service
  metadata:
   name: my-service
  spec:
   selector:
    name: myapp1
    ports:
     - protocol: TCP
       port: 8080
       targetPort: 80
  ------------------------------------
  * On the yml file we can also provide label under metadata
  NAME     READY   STATUS    RESTARTS   AGE    LABELS
  mypod1   1/1     Running   0          87m    name=myapp1
  mypod2   1/1     Running   0          117s   name=myapp1
  mypod3   1/1     Running   0          20s    color=red
  * About selectors -  if we would like to group two pods say mypod1 and mypod2, its not a good approch to group it with pod name, because they get change if we add more number of pods. As the key-value labels are identical, it is better to use this for grouping and add them in selector
  * port - is a service port, here we have given as 8080
  * targetPort - it is port at which the application is running in the container. So the containers in the pods which are running are at 80, therefore we give 80 here as well

  kubelet create -f svc.yml
  kubectl describe svc my-service
  now the my-service will group pods with identical labels and also the end-points of the two pods will be clubbed and shown for the above command. If we create another pod with same label then it will also join with this service
  kubectl get pods
  kubectl delete service <service_name>
  kubectl edit svc my-service
  kubectl get events -w     (get status of the pods)

kubectl edit node kmaster


Create POD in another way:
  In docker -e is used for passing the environmental variable
  vi msql.yml
  ----------------------------------------------------------
  apiVersion: v1
  kind: Pod
  metadata:
    name: mysql-pod
    labels:
      name: mysql-pod
  spec:
    containers:
      - name: mysql
        image: mysql:latest
        env:
          - name: "MYSQL_USER"
            value: "user1"
          - name: "MYSQL_PASSWORD"
            value: "centos"
          - name: "MYSQL_DATABASE"
            value: "simlilearn"
          - name: "MYSQL_ROOT_PASSWORD"
            value: "supercentos"
        ports:
          - containerPort: 3306
    -------------------------------------------------------
   kubctl create -f mysql.yml
   kubctl exec -it mysql-pod bash
   env
   env | grep -i user
   env | grep -i password
   mysql -uuser1 -pcentos
   show databases
   use simplilearn

kubectl delete pods mypod1
kubectl delete all -l='name=myapp1'
delete all the pods so far created

Deployment:
  kubectl run mydep1 --image=httpd    (run is depricated and it maynot be avaiable in future versions)
  kubectl get pods
  kubectl get deployment
  kubectl edit deployment mydep1
    change replicas here to 2
  kubectl get pods -w
  kubectl get pods -o wide
  kubectl delete pod <pod_name>
  kubectl get pods -w
  kubectl get deployment
    Now create a service on top of deployment without yml file
    kubectl expose deployment mydep1 --port=80   (or) kubectl expose deployment mydep1 --port=80 --name=deplyservice
    kubectl get svc
    curl <cluster_ip>
    delete the previous service if it is not necessary

    now create a service on top of deployment using yml file
    kubectl delete svc <service_name>
    kubectl get svc
    kubectl get deployment
    Kubectl get deployment --show-labels
    to create a service here we take label of deployment instead of pod label
    vi svc.yml
    ---------------------------------
    apiVersion: v1
    kind: Service
    metadata:
     name: my-service
    spec:
     selector:
      run: mydep1
      ports:
       - protocol: TCP
         port: 8080
         targetPort: 80
    -----------------------------------
    kubectl create -f svc.yml
    kubectl describe svc my-service
    Finally more than one service can run one deployment, if we to look at it practically, just rename the service in above yml file and run it again. but this is not necessary

    Dry run: another way of deployment, this is to auto-generate yml file and then run the deployment from the generated yml file
      kubectl run mydep2 --image=httpd --dry-run -o yaml > mydep2.yml
      kubectl get deployment   (we cannont see the above deployment here as it is a dry run)
      cat mydep2.yml | less     (This is to look at the yml file which got auto generated from the above dry-run deployment)
      kubectl create -f mydep2.yml
      kubectl get deployment

    MYSQL deployment:
      kubectl run mydb --image=mysql:5.6 --env MYSQL_USER=user1 --env MYSQL_PASSWORD=centos --env MYSQL_ROOT_PASSWORD=centos
      kubectl get deployment
      kubectl get pods -w
      we can just delete old pods running if they are not needed
      we cal also get the yml file from the current deployment
      kubectl get deployment mydb -o yaml > test1.yml

Secrets:
  For instance, in the above deployments we have some confidential parameters which is not good to expose everyone in the production Environment so use this
  It is encoded concept, not encrypted
  we have 3 types : docker-registry,   generic,  tls
  kubectl create secret generic mysecret --from-literal='db_pass'='centos'
  kubectl get secret
  kubectl edit secret mysecret
  now apply this secret to any deployment having confidential information
  kubectl edit deployment mydb
    go to the end of the yml file and apply this secret to MYSQL_ROOT_PASSWORD, remove the value line and enter this
    --------------------
    valueFrom:
      secretKeyRef:
        key: db_pass
        name: mysecret
    ---------------------
  kubectl get pods -w
  The moment we edit the deployment and pass the scerets then will recreate pods immediately

  Now create secret from source file, generally this approach will be used in production
    echo "password" > mypasswd.txt
    cat mypasswd.txt
    kubectl create secret generic passwdtxt --from-file=mypasswd.txt
    kubectl edit secret passwdtxt
    rm mypasswd.txt   (we can remove this file if not needed further)
    now we will create a pod which uses this secret generated from the file
    vi secpod.yml
    -----------------------------
    apiVersion: v1
    kind: Pod
    metadata:
      name: consumesec
    spec:
      containers:
      - name: shell
        image: centos:7
        command:
          - "bin/bash"
          - "-c"
          - "sleep 10000"
        volumeMounts:
          - name: apikeyvol
            mountPath: "/tmp/apikey"
            readOnly: true
      volumes:
      - name: apikeyvol
        secret:
          secreteName: passwdtxt
    -------------------------------------
    kubectl create -f secpod.yml
    kubectl get pods -w
    kubectl exec -it consumesec bash
    cat /tmp/apikey/mypasswd.txt
    mount | grep api

Configmap: This is same as like secret but the passed literals wont be in encoded way
  kubectl create configmap test --from-literal=mood=happy
  kubectl get configmap
  kubectl edit configmap test
  -------------------------------
  apiVersion: v1
  kind: Pod
  metadata:
    name: dapi-test-pod
  spec:
    containers:
      - name: test-container
        image: gcr.io/google_containers/busybox
        command: [ "/bin/sh", "-c", "env" ]
        env:
          - name: SPECIAL_LEVEL_KEY
            valueFrom:
              configMapKeyRef:
                name: special-config
                key: special.how
          - name: SPECIAL_TYPE_KEY
            valueFrom:
              configMapKeyRef:
                name: special-config
                key: special.type
        envFrom:
          - configMapRef:
              name: env-config
    restartPolicy: Never
    -------------------------------------
    TRY THIS IF YOU HAVE SOME FREE TIME

NodePort: Convert service's clusterIP to NodePort: by doing this the system which is out of the cluster can access the application hosted in any node
  Range of node port: 30000 to 32767
  kubectl run test --image=httpd
  kubectl get pods
  kubectl expose deployment test --port=80     (or)   kubectl expose deployment test --port=80 --name=mysvc2  --type=NodePort
  kubectl get pods
  kubectl get svc
  curl <cluster_ip>
  kubectl edit svc test
    under spec: change type from ClusterIP to NodePort
  kubectl get svc
    here we can observe in Ports column that some random IP is assigned for node
  cat /etc/hosts
    now taking the IP of either master or nodes and access the hosted application with the assigned node port
  We can also assign port manually instead of leaving it to kubernetes by editing the service and it spec
  Note: here the application should actually be accessed out of the cluster but that is not happening

Change namespace of cluster:
  kubectl get namespace
  kubectl describe namespace default
  kubectl delete namespace ns1
  kubectl create namespace ns2
  kubectl get pods
  kubectl config current-context
  cat .kube/config
  kubectl config set-context $(kubectl config current-context) --namespace=ns2
  kubectl config view | grep namespace
  kubectl get pods
  as the pods my have been working on default namespace so you maynot see any resorces
  kubectl create -f pod.yml
  if we want to switch back again to default namespace, just execute the below command
  kubectl config set-context $(kubectl config current-context) --namespace=default
  kubectl get pods

Create a image using docker file, push it to public or private registry(in the below demo we are pushing it to docker hub), and that image for pod creation
  Also we can do this by not pushing it to any remote repository, just leave the image in local repository
  mkdir Docker
  vi Dockerfile
  -----------------------------------------
  FROM docker.io/centos
  MAINTAINER "abcl abcl@simplilearn.com"
  ENTRYPOINT ["ping"]
  CMD ["google.com"]
  -----------------------------------------
  docker build -t ping .
    docker run ping   (to check the image output)
    docker run ping yahoo.com  (again this is to check the image output)
  now we need to tag the image before pushing image to docker hub
  docker tag ping:latest aviprod/testk8s:ping
  Now do docker login before pushing
  docker pushaviprod/testk8s:ping   (image will be push to docker hub)
  vi pod.yml
  ------------------------------
  apiVersion: v1
  kind: Pod
  metadata:
    name: myping
    labels:
      name: pingapp
  spec:
    containers:
    - name: pingcontainer
      image: aviprod/testk8s:ping
      ports:
      - container: 80
  -------------------------------
  kubectl create -f pod.yml
  kubctl get pods -o wide   (the image will get pushed in some nodes)
  just cross check by ssh to the node and execute below command
  docker images

  Second method: is to take the created image in the local and deploy a pod . Its not recommended
  docker save aviprod/testk8s/ping -o pingimage.tar
  ls
  scp pingimage.tar user_name@slave_node/home/user_name/   (save this in whichever node is this image doesnt have)
  ssh user_name@slave_node
  sudo -i
  docker load -i /home/user_name/pingimage.tar
  docker images

  Now about imagePullPolicy:  1. Always  2. IfNotPresent  3.Never
  cat pod.yml
  kubectl edit pods myping    (to check imagePullPolicy state)
  now delete the myping pod execute the demo
  kubectl delete pod myping
  vi pod.yml
  ------------------------------
  apiVersion: v1
  kind: Pod
  metadata:
    name: myping
    labels:
      name: pingapp
  spec:
    containers:
    - name: pingcontainer
      image: aviprod/testk8s:ping
      imagePullPolicy: Never
      ports:
      - container: 80
  -------------------------------
  kubectl create -f pod.yml
  kubectl get pods -w
  kubectl edit pod myping   (here you can see the imagePullPolicy parameter is set to Never)



etcd: it is a consistent and distributed key-value store used for discovering services. it stores all key-value pairs, nothing but it stores the meta-data of the entire cluster
      It can be configured externally, nothing but out of the cluster. etcd-master is running on kube-system namespace
  kubectl get pods --all-namespaces   (it runs in master machine)
  kubectl get pods --all-namespaces -o wide   (by this we can ensure that etcd is running on master node)
  kubectl exec -it etcd-kmaster ls -n kube-system
  kubectl exec -it -n kube-system etcd-kmaster -- ps aux    (It give the information that at which iP and port is it running)
  advertise_url="https://ip_address:port"
  echo $advertise_url
  cd /etc/kubernetes/pki/etcd   (This is directory which holds the crt and key pairs of the etcd. When we do kubeadm init to install the cluster, it will create all these things )
  kubectl exec -it -n kube-system etcd-kmaster -- sh -c "ETCDCTL_API=3 etcdctl --endpoints $advertise_url --cacert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/server.key --cert /etc/kubernetes/pki/etcd/server.crt snapshot save test.db"
  kubectl exec -it -n kube-system etcd-kmaster -- sh -c "ETCDCTL_API=3 etcdctl --endpoints $advertise_url --cacert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/server.key --cert /etc/kubernetes/pki/etcd/server.crt member list"
  kubectl exec -it -n kube-system etcd-kmaster -- sh -c "ETCDCTL_API=3 etcdctl --endpoints $advertise_url --cacert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/server.key --cert /etc/kubernetes/pki/etcd/server.crt get \"\" --prefix=true -w json" > etcd.json    (here get \"\"\ means  --ALL RESOURCES)
  cat etcd.json
  vi etcd.json
  apt-get instal jq
  for k in $(cat etcd.json | jq '.kvs[].key' | cut -d '"' -f2); do echo $k | base64 --decode; echo; done    (this will decode the encripted information in etcd.json)
  for k in $(cat etcd.json | jq '.kvs[].key' | cut -d '"' -f2); do echo $k | base64 --decode; echo; done > count.json   (this copies all the info to count.json)
  cat count.json | wc -l    (this prints the count of the lines)
  for k in $(cat etcd.json | jq '.kvs[].key' | cut -d '"' -f2); do echo $k | base64 --decode; echo; done | grep mysql  (with this we can get pod info)
  for k in $(cat etcd.json | jq '.kvs[].key' | cut -d '"' -f2); do echo $k | base64 --decode; echo; done | grep pod_id  (this will get info of individual pod keys)

  Now choose particular key from the above for loop command and get the resources out of it
  for k in $(cat etcd.json | jq '.kvs[].key' | cut -d '"' -f2); do echo $k | base64 --decode; echo; done
  KEY="/registry/services/specs/default/mario"    (the key assigned to it is for example, please choose yours)
  echo $KEY
  kubectl exec -it -n kube-system etcd-kmaster -- sh -c "ETCDCTL_API=3 etcdctl --endpoints $advertise_url --cacert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/server.key --cert /etc/kubernetes/pki/etcd/server.crt get \"$KEY\" -w json" | jq    (here get \"\"\ means  --ALL RESOURCES)
  use play.etcd.io/play

Kube-api Server
  It is the front-end of the Kubernetes which also exposes the Kubernetes API. It is a gateway to the Kubernetes cluster that implements a RESTful API and HTTP
  This again runs on kube-system namespace. One of the important feature is it authorizes and authenticates various  mechanisms, using this we can function RBAC
  Kubeproxy will be present every node in the cluster. Kubeproxy has the information of the entier pod
  We have two components: 1. API Group   2. API Versioning
  kubectl get pods --all-namespaces -o wide
  kubectl api-versions

Controller Manager:
  The components of kubernetes that are used to manage the non-terminating control loops and regulate the kubernetes cluster
  Two types of controller managers: 1. kube-controller-manager    2. cloud-controller-manager
  Type of kube-controller-manager:  1. Node controlling 2. Endpoint controlling  3. Replica Controller   4. Service account and token controller
    kubectl get sa/serviceaccount     (it is a user creation or RBAC)
  Types of cloud-controller-manager:  1. Node controller  2. Route controller    3. Volume controller     4. Service  controller

Kubernetes Scheduler:
  It is a component of the master node that is reponsible for scheduling tasks for the slave nodes and storing resorce information of each node
  Two operations of kube-sheduler is:   Filtering and scoring

Kubelet:
  Kubelet is daemon which runs in every node

KubeProxy:
  It is a network proxy and a load balancer that runs on each node of the cluster and is responsible for forwarding requests

Pod:
  It is the smallest unit of an application that represents a process running on the cluster

Problem Statement:
  Create the following
    Pods using the YAML file
    Pods using the command kubectl
    Pods on specific namespace

ReplicaSet:
  It is a method used to maintain a set of replica pods running and to identify the identical pods
  When to use Replicaset:
    When we require custom update Orchestration
    When you dont require updates at all
  kubectl get replicaset
  kubectl explain replicaset
  vi rc.yml
  -----------------------------------
  apiVersion: extensions/v1beta1
  kind: ReplicaSet
  metadata:
   name: mysql
   # labels so that we can bind a Service to this Pod
   labels:
    app: mysql
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: mysql
    template:
      metadata:
        labels:
          app: mysql
      spec:
        containers:
        - name: database
          image: mysql
          resources:
            requests:
              cpu: 1
              memory: 2Gi
          env:
          - name: MYSQL_ROOT_PASSWORD
            value: some-password-here
          ports:
          - containerPort: 3306
  --------------------------------------------
  kubectl create -f rc.yml
  kubectl get replicaset    (we can see one more replicaset now)
  kubectl get pods
  kubectl edit replicaset mysql   (increase the replicas to 2)
  kubectl get pods
  Try this problem description:
    You are given a project to implement the following:
      Create a ReplicaSet using the YAML file
      Create a ReplicaSet using the command kubectl


Deployment:
  A deployment is a controller that provides updates for pods and ReplicaSet. The main role of the deployment is to change the actual state to the desired state as specified


4th Video:
  kubectl delete all -all   (which cleanup the complete resorces)
  systemctl status kubelet    (make sure this should be in running state as if any node goes down this will take care of joining it again)
  kubectl describe pod etcd-kmaster -n kube-system
Deployment Demo:
  vi dc.yml
  -------------------------------------------
  apiVersion: apps/v1  #for versions before 1.9.0 use apps/v1beta2
  kind: Deployment
  metadata:
    name: nginx-deployment
  spec:
    selector:
      matchLabels:
        app: httpd
    replicas: 2   # tells deployment to run 2 pods matching the template
    template:
      metadata:
        labels:
          app: httpd
      spec:
        containers:
        - name: httpd
          image: httpd:latest
          ports:
          - containerPort: 80
    ---------------------------------------------------
    kubectl create -f dc.yml
    kubectl get deployment
    kubectl get replicasets
    kubectl edit deployment nginx-deployment    (here we can change the replicaset count)
    kubectl get pods -o wide
    curl ip_address_of_pod    (of both pods)

    Deployment through run command, not from yml file
      kubectl delete deployment nginx-deployment  (wait until all pods get terminated)
      kubectl run httpd --image=httpd --image-pull-policy=IfNotPresent --replicas=2

    Deployment through dry run
      kubectl run httpd --image=httpd --image-pull-policy=IfNotPresent --replicas=2 --dry-run -o yaml > mydc.yml

Services and Service ClusterIP:
  A service is a way of exposing an application that runs on a set of pods as a network service
  To do so, pods are given an IP address and a DNS name, through which they are being load balanced
    When you want to map a service to a deployment you have to consider a deployment label not the pod level
  Here are the access types in services:
    ClusterIP - The packets sent are never NAT'd      (NAT - Network address translation)
      the process of recognizing the POP IPs by the service is known as endpoint
    NodePort  - The packets sent are NAT'd by default
    LoadBalancer  - The packets sent are NAT'd by default
      kubectl get svc -n name_space
      Below is the steps how the end user can access the application
      DNS--> PUBLIC---> LAPTOPS PRIVATE
        1.DNS--> NODEIP:NODEPORT ---> POD
        2.DNS--> Physical LoadBalancer --> Service
    dmidecode system-manufacturer | grep -i ec2

  Create a service using yaml file:
  vi pod.yml
  ----------------------------------
  apiVersion: v1
  kind: Pod
  metadata:
   name: apache
   labels:
    name: myapp1
  spec:
   containers:
   - name: mycontainer
     image: docker.io/httpd
     imagePullPolicy: Always
     ports:
     - containerPort: 80
  ----------------------------------
  kubectl create -f pod.yml
  kubectl get pods
  vi svc.yml
  ----------------------------------
  apiVersion: v1
  kind: Service
  metadata:
   name: my-service
  spec:
   selector:
    name: myapp1
    ports:
     - protocol: TCP
       port: 8080
       targetPort: 80
  ------------------------------------
  kubectl create -f svc.yml
  kubectl describe svc myservice
  We can create another pod with same label and check IP assignment in describe command

  Another way of creating service is "kubectl expose" command
  The last type is if we have deployment already then on top we can create a service from file but the condition here is the selectors will be for deployment not for pods

Create a multi-tier application:
  kubectl create namespace twotier
  kubectl run mysql --image=mysql:5.6 --env MYSQL_USER=user1 --env MYSQL_PASSWORD=redhat --env MYSQL_DATABASE=blr --env MYSQL_ROOT_PASSWORD=redhat -n twotier
  kubectl get pods -w -n twotier
  kubectl get deployments -n twotier
  kubectl expose deployment mysql --name=mydb -n twotier --port=3306  (choose which port should it be run)
  kubectl get svc -n twotire
  kubectl run wordpress --image=wordpress --port=80 --env WP_HOST=mydb --env WP_PASSWORD=redhat -n twotier  (WP_HOST is the parameter takes us to the database connectivity)
  kubectl get pods -w -n twotier
  kubectl describe pod pod_id -n twotier
  kubectl expose deployment wordpress --port=80 -n twotier
  kubectl get svc -n twotier
  kubectl edit svc wordpress -n twotier
  change the type under spec to NodePort
  curl wgetip.com
  take the public IP and port to browse the application
  Once the wordpress application is accessible use this inputs to initialize
    Database Name: blr
    Username: root
    Password: redhat
    Database Host: mydb
    Table Prefix: can be anything
    Enter Submit
    Run the Installation
    Site Title: MYCKA
    Username: user1
    Password: redhat
    Confirm use of weak password
    Your Email: a@a.com
    intall word press
    Now do login with user1|redhat
    Write your first post
    Publish
    Publish
    View Post

Manual Scheduling:
  It is possible to schedule a pod manually, without using a sheduler, by specifying it in spec.nodeName. It can also be done using DaemonSet
  Binding Concept:
    Binding is the process in which the cluster operator is bound with the managed service to acquire the account details and connection credentials of the application
    It uses ServiceBinding resorces which requests for all necessary information to bind the service instance
  vi nodename.yml
  --------------------------
  apiVersion: v1
  kind: Pod
  metadata:
    name: nodename
    labels:
      env: test
  spec:
    containers:
    - name: nodename
      image: nginx
      imagePullPolicy: IfNotPresent
    nodeName: kslave2
 ------------------------------------
 kubectl create -f nodename.yml
 kubectl get pods -o wide

Labels and Selectors:
  labels are key-value pairs used to define attributes of an object. They can be specified when modified values are created, if required
  labels can be defined for a nodes and selector can be defined for pods definition
  Tow types of Selectors are there:
  1. Equity-based selector    2. Set-based selector

  kubectl get nodes --show-labels
  kubectl label node kslave1 color=blue
   kubectl get nodes --show-labels
  vi nodeselector.yml
  ---------------------------------------
  apiVersion: v1
  kind: Pod
  metadata:
    name: nodeselector
    labels:
      env: test
  spec:
    containers:
    - name: nodeselector
      image: nginx
      imagePullPolicy: IfNotPresent
    nodeSelector:
      color: blue
  ---------------------------------------
  kubectl create -f nodeselector.yml
  kubectl get pods -w -o wide
  This type of assigning a pod to a selected node is Node Affinity
  kubectl get pods -l env=test    (here env=test is equity-based query, and l stands for labels)
  kubectl get pods -l 'env in (test)'   (here in and notin are set-based query)
    kubectl get pods -l 'env noin (test)'
    kubectl get pods -l 'env notin (test), color in (blue)' --show-labels

  Create two pods with same label and attach it to the service
  for example say env: test
  vi labels.yml
  -------------------------------------
  apiVersion: v1
  kind: Service
  metadata:
    name: label-service
  spec:
    selector:
      app: MyApp
    ports:
      - protocol: TCP
        port: 80
        targetPort: 9376
  ----------------------------------------
  kubectl create -f labels.yml
  kubectl describe svc label-service
  kubectl edit svc label-service
    change a key-value combination of a selector under spec to env: test
  kubectl describe svc label-service
    now we can observe pods mapped to this service

Create pods on master node
  kubectl edit nodes kmaster
    remove the taints: under the spec:
    -------------------------------------------
    taints:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
   --------------------------------------------
    Doing this will now create pods under master node as well, nothing but the master node can be schedulable
  Now create pods using ReplicaSet in kmaster and kslave1
  vi repms.yml
  -------------------------------------------------
  apiVersion: extensions/v1beta1
  kind: ReplicaSet
  metadata:
    name: master-slave
  spec:
    replicas: 2
    template:
      metadata:
        name: master-slave
        namespace: default
        labels:
          env: ms
      spec:
        containers:
        - name: master-slave
          image: nginx
          imagePullPolicy: IfNotPresent
          ports:
          - containerPort: 80
        nodeSelector:
          color: blue
  ---------------------------------------------
  kubectl create -f repms.yml
  kubectl get replicaset
  kubectl get pods -l env=ms -o wide







following commands appear when executing kubeadm reset
  You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'

Topics to cover:
  1. Kubernetes the hard way
      https://www.youtube.com/watch?v=NvQY5tuxALY
  2. Kubernetes service providing stuff
  3. what is wildcards in service creation
  4. RBAC - Roll based access control
  5. Try this https://vitux.com/install-and-deploy-kubernetes-on-ubuntu/



Doubt:
  1. Difference between ClusterIP and NodePort
  2. After exposing the nodeport, i dont understand how to access the application from outside cluster
  3. Difference between docker.io and docker hub
  4. Did not understand ReplicaSet
      https://www.youtube.com/watch?v=1WM-LsH6tKc
